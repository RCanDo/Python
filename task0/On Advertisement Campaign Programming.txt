On Advertisement Campaign Programming
=====================================

By _programming_ we mean design of strategy along with gathering and analysing data
in order to adjust this strategy.

It means that these three components work in a cycle of continuous refinement
where all sorts of parameters are fine tuned based on historical and real time data analysis.

General viewpoint
-----------------

The problem of proper targeting of advertisment or marketing campaign
may be approached as a task of finding out most profitable matches
between elements of the set of product ads and elements of the set of users.
Thus application of _recommender system_ methodology seems to be straightforward and immediate.
The profitable match may be viewed as voting _yes_ (_thumb up_)
while non-profitable as voting _no_ (_thumb down_).

Notice that for every user we search for some small number of most relevant products (ads),
while this number is usually assumed to be greater then one.
Such a task of _matching_ may be also formalised as the task of _filtering_:
instead of exposing all products to every user we filter out only those which are relevent
(most profitable) for a given user.

However, the task of matching here is symmetric hence it is also sensible,
and probably more appropriate, to think about filtering out those user groups which would be
most responsive to advertisement of a given product.

What is specific in finding proper target for advertisement campaigns,
and makes the task more difficult, is that _profitability of match_
is not easy to assess as there is no direct _yes/no_ voting option under advertisements.
More on this below.

Two basic approaches
--------------------

There are various approaches to the _recommendation_ (_matching_, _filtering_) problem,
nevertheless all of them have the same main objective:
similar products should be matched to similar users based on history of matches (filterings),
i.e. good (most profitable) matches are repeated while bad (not profitable) matches are avoided.

What is variable between methods is a specific methodology for finding out groups of similarity
for users and products.
Two basic approaches to the _recommendation_ task
(described here in the context of advertisement campaign) are:

1. __collaborative filtering__
   where a single user history is used to find out similar users i.e.
   those who behaved in a similar way in the past.  
   We assume that this group has already "voted" somehow for a given ad so we may predict that our
   user will most certainly follow the most common decision within the whole group.  
   This method may be seen as for a single user finding out an _ad hoc similarity group_
   (in terms of its historical behaviour) with the constraint that
   each member of this group have already "voted" for a given product ad.
   
2. __content-based filtering__
   where we do not rely on the single user history but rather on already well established
   groups of users with similar preferences (or needs or other characteristics).  
   It means that we do __not__ create an _ad hoc similarity group_ for a given user but rather
   work with groups already defined and only find out to which predefined group the user belongs.  
   This method seems to be better suited fot the advertisement campaign as working with
   single user, as in _collaborative filtering_, is definitely more cumbersome if at all
   feasable.

Whatever method we choose we always deal with two problems:

1. The data with which we begin our campaign necessary for good grouping.
2. The method of assessing profitability of a match (between user and product).

Data for the start
------------------

In order to figure out what _similarity groups_ of users we could work with
we need to have good records of the users history.

It would be very difficult to begin any campaign having no information about users preferences and needs
which can be figured out effectively only from their history and other personal data.
In such a case some randomly driven experimentation is necessary just to collect some data for preliminary
grouping which should be evaluated and refined in the course of campaign with the aid of new data.

Polls and surveys could be also applied to gather any information valid for preliminary grouping.

Assessing efficiency
--------------------

Having good assesment of the profitability of match (between user and product) is crucial.
As already described, we do not usually know what is the effect of the given ad for the given user
as there is no direct voting available under ads.
We may only infere the effect indirectly from other sources,
like growth in selling of the advertised product or users behaviour after ad was presented to her/him.
This poses a serious problem as the response of a specific user (group)
is hard to measure and defered in time.

Having access to information if the user purchased the product from the ad
(within _some_ time period - another uncertainty) would be the perfect solution.
Such a variable would be just 0/1 score (failure/success) which then would serve
as a well defined target variable - objective for most profitable matching in the future.

Alas, it seems to be much more common that we do not have this very information, i.e.
we are not provided with the exact answer to the question if the user finally purchased the product or not.

Instead we may e.g. observe if the user interest (infered from the user acitvity)
in some kind of product has phased out after given ad was presented to her/him.
We would not be certain why exactly this happened but we still may ascribe to this user
some probability (continuous score) of the given ad being effective.
The exact value of this score would also depend on other relevant variables.
As in the 0/1 case we obtain a proper target variable for making future predictions
on ads efficiency for given sort of users.


User grouping
-------------

Regardless of the type of grouping,
_ad hoc_ for every single user or _in advance_ grouping based on the whole data available,
we may ponder over what sort of data would be good for such a grouping.
The general answer is that probably everything we could gather,
but some sort of information would necessarily be more important then other.

E.g. _users interest in the product_ itself is good (probably the best possible)
indicator of the ad content which would be most profitable to be presented to this user.
This is grouping steered by the product
- splitting all users into small number of groups wrt the level of their interest in the product,
infered from their recent activity.
This kind of grouping has advantage of not being dependent on a long history records
as only most recent users activity is relevant.
Good timing here is crucial as being late with an ad (user already satisfied their needs)
is annoying and completely ineffective thus a waste of resources.

On the other hand we may think about grouping wrt longer users history which would allow to
create overall and in depth (to some extent) characteristics of each user
- this in turn allows finer and more accurate grouping.
The advantage here is that it gives us good clues for advertising products not necessarily
in the scope of the current interest of the user,
i.e. we may suggest completely new products the user may be even not aware they exist.

Of course, both approches may be mixed what should result in better efficiency
then if applied separately.

Product grouping
----------------

In both cases the assumption that we consider one specific product may be relaxed
as we may also think about product categories i.e. groups of products similar in some way.
Notice that _product grouping_ is easier than _user grouping_ as it is more of a conceptual work
than data analysis.
It may be done reasonably well based on product specifications and common sense.
Nevertheless, it may be still a challenge, just because of the size of the task,
hence some data science techniques may be indispensable.

Reinforcement learning
----------------------

It is well known that the methodology of _reinforcement learning_ may be succesively applied
for the _recommender systems_.
The rationale is that every iteration of _matching_ brings new information about profitability
of particular matches.
The interesting thing is that we may begin with no previous experience at all -
the algorithm will improve its efficiency successively incorporating new experience
via updating all the relevant parameters accordingly.
We may begin with randomly chosen parameters when we have no clues whatsoever.
Of corse it's always better to begin with anything as long as the information provided is reliable.

The part of this approach is well known experimentation/exploitation dillema:
the only way of acquiring knew information and knowledge about the environment (users-products "ecosystem")
is random serching for new solutions (_matches_ which were not excercised yet).
Most of them would be non-profitable but some of them may happen to be non-obvious good _matches_.
Of course, experimentation is expensive (we usually end up with wrong _matches_)
however is necessary for the whole system to learn new solutions.

Summary
-------

The topic is very broad and a lot of aspects must be considered for final design.
The most important conclusion is that any solution must involve iterative approach
where continuous gathering of new data together with their analysis
and distilling knowledge from them are the integral part of the whole system.

The crucial task and the biggest challenge is proper design of the _target_ variable
(also called _reward_ in _reinforcement learning_ framework), i.e. assessing
profitablility of the _matching_ between users and product ads.

